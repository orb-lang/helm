* Helm Database


  This module provides all the mechanisms for CRUDing the helm SQLite
database.


**** imports

#!lua
local uv  = require "luv"
local sql = assert(sql, "sql must be in bridge _G")
local Arcivist = require "sqlun:arcivist"
#/lua


** helm_db

#!lua
local helm_db = {}
#/lua


**** helm_db_home

#!lua
local helm_db_home =  (os.getenv 'HELM_HOME'
                      or _Bridge.bridge_home) .. "/helm/helm.sqlite"
helm_db.helm_db_home = helm_db_home
#/lua


**** _conns

A weak table to hold conns, keyed by string path.

This does mean we can only have one in-memory database =""= at a time, which
should be okay.

#!lua
local _conns = setmetatable({}, { __mode = 'v' })
#/lua


**** _resolveConn(conn)

A helper function to retrieve a conn if we already have one.

Doesn't build a conn if it can't find one, and presumes that non-string
parameters are already conns.

#!lua
local function _resolveConn(conn)
   if conn then
      if type(conn) == 'string' then
         return _conns[conn]
      else
         return conn
      end
   end
   return nil
end
#/lua


**** _openConn(conn_handle?)

Returns an open conn, defaults to =helm_db_home=, will reuse existing conns if
it has them.

#!lua
local function _openConn(conn_handle)
   if not conn_handle then
      conn_handle = helm_db_home
   end
   local conn = _resolveConn(conn_handle)
   if not conn then
      conn = helm_db.boot(conn_handle)
   end
   assert(conn, "no conn! " .. conn_handle)
   return conn
end
#/lua


*** Create tables

The schema with the highest version number is the one which is current for
that table.  The table will of course not have the =_n= suffix in the
database.  The number is that of the migration where the table was recreated.

Other than that, SQLite lets you add columns and rename tables.

When this is done, it will be noted.


**** Canonical

These are the current forms of each table.


***** Project

#!sql @create_project_table_3 #asLua
CREATE TABLE IF NOT EXISTS project_3 (
   project_id INTEGER PRIMARY KEY AUTOINCREMENT,
   directory TEXT UNIQUE,
   time DATETIME DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now'))
);
#/sql


***** Repl

#!sql @create_repl_table_3 #asLua
CREATE TABLE IF NOT EXISTS repl_3 (
   line_id INTEGER PRIMARY KEY AUTOINCREMENT,
   project INTEGER,
   line TEXT,
   time DATETIME DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now')),
   FOREIGN KEY (project)
      REFERENCES project (project_id)
      ON DELETE CASCADE
);
#/sql


***** Result

#!sql @create_result_table #asLua
CREATE TABLE IF NOT EXISTS result (
   result_id INTEGER PRIMARY KEY AUTOINCREMENT,
   line_id INTEGER,
   repr text NOT NULL,
   value blob,
   FOREIGN KEY (line_id)
      REFERENCES repl (line_id)
      ON DELETE CASCADE
);
#/sql


***** Session

#!sql @create_session_table_4 #asLua
CREATE TABLE IF NOT EXISTS session (
   session_id INTEGER PRIMARY KEY,
   title TEXT,
   project INTEGER,
   accepted INTEGER NOT NULL DEFAULT 0 CHECK (accepted = 0 or accepted = 1),
   vc_hash TEXT,
   FOREIGN KEY (project)
      REFERENCES project (project_id)
      ON DELETE CASCADE
);
#/sql


***** Premise

#!sql @create_premise_table #asLua
CREATE TABLE IF NOT EXISTS premise (
   session INTEGER NOT NULL,
   line INTEGER NOT NULL,
   -- ordinal is 1-indexed for Lua compatibility
   -- "ordinal" not "order" because SQL
   ordinal INTEGER NOT NULL CHECK (ordinal > 0),
   title TEXT,
   status STRING NOT NULL CHECK (
      status = 'accept' or status = 'reject' or status = 'ignore' ),
   PRIMARY KEY (session, ordinal) ON CONFLICT REPLACE
   FOREIGN KEY (session)
      REFERENCES session (session_id)
      ON DELETE CASCADE
   FOREIGN KEY (line)
      REFERENCES repl (line_id)
      ON DELETE CASCADE
);
#/sql


**** Obsolete

These are old forms of tables, which we need in order to properly migrate.

#!sql @create_project_table #asLua
CREATE TABLE IF NOT EXISTS project (
   project_id INTEGER PRIMARY KEY AUTOINCREMENT,
   directory TEXT UNIQUE,
   time DATETIME DEFAULT CURRENT_TIMESTAMP
);
#/sql

#!sql @create_session_table #asLua
CREATE TABLE IF NOT EXISTS session (
session_id INTEGER PRIMARY KEY AUTOINCREMENT,
name TEXT,
project INTEGER,
-- These two are line_ids
start INTEGER NOT NULL,
end INTEGER,
test BOOLEAN,
sha TEXT,
FOREIGN KEY (project)
   REFERENCES project (project_id)
   ON DELETE CASCADE );
#/sql

#!sql @create_repl_table #asLua
CREATE TABLE IF NOT EXISTS repl (
   line_id INTEGER PRIMARY KEY AUTOINCREMENT,
   project INTEGER,
   line TEXT,
   time DATETIME DEFAULT CURRENT_TIMESTAMP,
   FOREIGN KEY (project)
      REFERENCES project (project_id)
      ON DELETE CASCADE
);
#/sql


*** Migrations

  We follow a simple format for migrations, incrementing the =user_version=
pragma by 1 for each alteration of the schema.

We write a single function, which receives the database conn, for each change,
such that we should be able to take a database at any schema and bring it up
to the standard needed for this version of =helm=.

We store these migrations in an array, such that =migration[i]= creates
=user_version= =i=.

We skip =1= because a pragma equal to 1 is translated to the boolean =true=.

Migrations return =true=, in case we find a migration that needs a check to
pass.  Unless that happens, we won't bother to check the return value.

#!lua
helm_db.HELM_DB_VERSION = 3

local migrations = {function() return true end}
helm_db.migrations = migrations
#/lua


**** Version 2: Creates tables.

#!lua
local insert = assert(table.insert)

local migration_2 = {
   create_project_table,
   create_result_table,
   create_repl_table,
   create_session_table
}

insert(migrations, migration_2)
#/lua


**** Version 3: Millisecond-resolution timestamps.

  We want to accomplish two things here: change the format of all existing
timestamps, and change the default to have millisecond resolution and use
"T" instead of " " as the separator.

SQLite being what it is, the latter requires us to copy everything to a new
table.  This must be done for the =project= and =repl= tables.

#!lua
local migration_3 = {}
#/lua

#!sql @migration_3[1] #asLua
UPDATE project
SET time = strftime('%Y-%m-%dT%H:%M:%f', time);
#/sql

#!lua
migration_3[2] = create_project_table_3
#/lua

#!sql @migration_3[3] #asLua
INSERT INTO project_3 (project_id, directory, time)
SELECT project_id, directory, time
FROM project;
#/sql

#!sql @migration_3[4] #asLua
DROP TABLE project;
#/sql

#!sql @migration_3[5] #asLua
ALTER TABLE project_3
RENAME TO project;
#/sql

#!sql @migration_3[6] #asLua
UPDATE repl
SET time = strftime('%Y-%m-%dT%H:%M:%f', time);
#/sql

#!lua
migration_3[7] = create_repl_table_3
#/lua

#!sql @migration_3[8] #asLua
INSERT INTO repl_3 (line_id, project, line, time)
SELECT line_id, project, line, time
FROM repl;
#/sql

#!sql @migration_3[8] #asLua
DROP TABLE repl;
#/sql

#!sql @migration_3[9] #asLua
ALTER TABLE repl_3
RENAME to repl;
#/sql

#!lua
insert(migrations, migration_3)
#/lua


**** Version 4: Sessions


***** Sessions

  The current session table is a stub, and isn't useful as is; nor is it
referred to anywhere in the codebase.

So the first step is to drop that table, and replace it with a new one which
does.

We add a second table, =premise=, for each line of a given session.  This way,
a session doesn't have to be a single block of line_ids (which is brittle),
but maintains its own order.

So running a session as a test, we load all the premises into a clean
environment, executing them in order, and checking the results against the
retrieved database values. If a premise marked as accepted = true changes, we
throw an error and (eventually) provide a diff.

We'll want to add additional affordances for easy testing, which will be
documented elsewhere.

#!lua
local migration_4 = {}
#/lua

#!sql @migration_4[1] #asLua
DROP TABLE session;
#/sql

#!lua
insert(migration_4, create_session_table_4)
insert(migration_4, create_premise_table)

insert(migrations, migration_4)
#/lua


**** Version 5

The primary change in this migration is to store results uniquely by hash.

#!lua
local migration_5 = {}
insert(migrations, migration_5)
#/lua


***** Simple Migrations

  "Simple" in the sense that they require little-to-no changes to the
applications.

The name =repl= for our collection of lines was never great, and =line= is a
reserved word in SQL, so we're already pushing our luck by having a =.line=
column.  So we'll rename to =input=, which is simple:

#!sql @migration_5[1] #asLua
ALTER TABLE repl RENAME TO input;
#/sql

Another thing we should be adding is an index for dates on lines, like so:

#!sql @migration_5[2] #asLua
CREATE INDEX idx_input_time ON input (time);
#/sql

This is a good idea anyway, since lines are our most expensive DB call during
startup, and with sessions, lines will be placed out of order.

We need to add an =AUTOINCREMENT= to the session table to get a stable
ordering while allowing deletions, and adding constraints means a full copy:

#!sql @create_session_table_5 #asLua
CREATE TABLE IF NOT EXISTS session_5 (
   session_id INTEGER PRIMARY KEY AUTOINCREMENT,
   title TEXT,
   project INTEGER,
   accepted INTEGER NOT NULL DEFAULT 0 CHECK (accepted = 0 or accepted = 1),
   vc_hash TEXT,
   FOREIGN KEY (project)
      REFERENCES project (project_id)
      ON DELETE CASCADE
);
#/sql

We'll move this to the top once this migration is complete; it's more
important to have an at-a-glance view into the schema, and we don't have
transclusions yet.

#!lua
migration_5[3] = create_session_table_5
#/lua

#!sql @migration_5[4] #asLua
INSERT INTO session_5(title, project, accepted, vc_hash)
SELECT title, project, accepted, vc_hash FROM session
;
#/sql

#!sql @migration_5[5] #asLua
DROP TABLE session;
#/sql

#!sql @migration_5[6] #asLua
ALTER TABLE session_5 RENAME TO session;
#/sql


***** hashing and de-duplication of results (w. truncation)

We want to start storing results as the hash of the repr string, and use that
as the foreign key into a new table, =repr=.

There are a number of circumstances where we will only need the hash, and by
making the hash column of =repr= a =UNIQUE=, we can get deduplication
without needing to handle it in-memory.

This does mean we have to pull every result, hash it, update the result
foreign key to point to the result, and commit to a new table.

While we're at this, it might pay off to truncate our absurdly long results.

It's likely I have the only database which /has/ a large result collection, so
maybe not worth the extra complexity? But I have a GB or so of data in my helm
DB, and it's not /that/ much extra work.

One minor optimization: we don't actually have to hash results which are
smaller than 64 bytes, the length of our hash.  We can simply treat the repr
as the hash, and commit it twice to the database, and this will save some tiny
amount of space.  There are circumstances when we pull the repr (and hence the
hash) without pulling the result right away, and if the result is less than 63
bytes, we know we don't have to bother with the second database call.

I'm not convinced this is worth doing, but it doesn't have any obvious
downsides.  The important part is that we can completely ignore this logic if
we want to, and get the same result, as long as we bake the conditional
hashing into a function, and use that function everywhere we hash: and we're
truncating anyway, relative to stock sha3-512.

For reference, here's the current schema of =result=:

#!sql
CREATE TABLE IF NOT EXISTS result (
   result_id INTEGER PRIMARY KEY AUTOINCREMENT,
   line_id INTEGER,
   repr text NOT NULL,
   value blob,
   FOREIGN KEY (line_id)
      REFERENCES repl (line_id)
      ON DELETE CASCADE
);
#/sql

Our new version looks like this:

#!sql @create_result_table_5 #asLua
CREATE TABLE IF NOT EXISTS result_5 (
   result_id INTEGER PRIMARY KEY AUTOINCREMENT,
   line_id INTEGER,
   hash text NOT NULL,
   FOREIGN KEY (line_id)
      REFERENCES input (line_id)
      ON DELETE CASCADE
   FOREIGN KEY (hash)
      REFERENCES repr (hash)
);
#/sql

We never used the value blob for anything, so we can just ignore it.

This does leave us with an exception to our schema rules, namely, we keep
calling the primary key on what is now =input= the =line_id=, instead of
=input_id= on the master table and =input= elsewhere.  A wart, but an
acceptable one.

And we need our =repr= table as well:

#!sql @create_repr_table #asLua
CREATE TABLE IF NOT EXISTS repr (
   hash TEXT PRIMARY KEY ON CONFLICT IGNORE,
   repr BLOB
);
#/sql

Which should have an index on hash:

#!sql @create_repr_hash_idx #asLua
CREATE INDEX repr_hash_idx ON repr (hash);
#/sql

Which we'll add to the migration in a Lua block, so we can move these
schema-defining operations to the top of the file.

#!lua
migration_5[7] = create_result_table_5
migration_5[8] = create_repr_table
migration_5[9] = create_repr_hash_idx
#/lua

Now for the fun part: we need a function which will create the new table,
select every line, hash the =repr= field, write the new hash and repr to
=repr=, and write the rest of the contents to =result_5=, then drop =result=
and rename.

We want the results in order of entry:

#!sql @get_old_result_5 #asLua
SELECT result_id, line_id, repr
FROM result
ORDER BY result_id
;
#/sql

To put it back:

#!sql @insert_new_result_5 #asLua
INSERT INTO result_5 (result_id, line_id, hash) VALUES (?, ?, ?);
#/sql

To write the hash:

#!sql @insert_repr_5 #asLua
INSERT INTO repr (hash, repr) VALUES (?, ?);
#/sql

Letting our ON CONFLICT IGNORE perform deduplication.

Then drop and rename:

#!sql @drop_result_5 #asLua
DROP TABLE result;
#/sql

#!sql @rename_result_5 #asLua
ALTER TABLE result_5 RENAME TO result;
#/sql

Now we tie it all together in a function.  =sha= is required locally, because
migrations are run seldom; by the nature of =valiant= and =helm=, we'll need
the function elsewhere, but this is good practice.

#!lua
local TRUNCATE_AT = 1048576 * 4 -- 4 MiB is long enough for one repr...

local function _truncate_repr(repr)
   local idx = TRUNCATE_AT
   if repr:sub(1, 1) == "\x01" then
      -- If this is a tokenized-format repr, look for the start of
      -- the next token after the 4MB mark, and stop just before it.
      -- Theoretically there might not be any such, if the repr is just
      -- barely over 4MB, in which case we keep the whole thing.
      idx = repr:find("\x01", idx, true)
      if idx then
         idx = idx - 1
      end
   end
   return repr:sub(1, idx)
end

migration_5[10] = function (conn, s)
   local sha = require "util:sha" . shorthash
   local insert_result = conn:prepare(insert_new_result_5)
   local insert_repr = conn:prepare(insert_repr_5)
   s:chat "Hashing results, this may take awhile..."
   local truncated = 0
   for _, result_id, line_id, repr in conn:prepare(get_old_result_5):cols() do
      ---[[
      if #repr > TRUNCATE_AT then
         s:verb("Found a %.2f MiB result!", #repr / 1048576)
         truncated = truncated + 1
         repr = _truncate_repr(repr)
      end
      --]]
      local hash = sha(repr)
      insert_result :bind(result_id, line_id, hash)
                    :step()
      insert_result :clearbind() :reset()
      insert_repr :bind(hash, repr) :step()
      insert_repr :clearbind() :reset()
   end
   s:chat("Truncated %d results", truncated)
   s:verb(drop_result_5)
   s:verb(rename_result_5)
   conn:exec(drop_result_5)
   conn:exec(rename_result_5)
   return true
end
#/lua

This reduces the size of my database from 1.6GB to 70MB.  It's unclear whether
this will actually reduce the size of databases in general, since mine was
dominated by excessively large prints of the environment from the uncompressed
interregnum of results storage.  It seems likely, but it triples the size of
small, unique values, and only shrinks the storage requirement for larger
prints which happen more than once.

But I must have a few hundred copies of =uv= alone in there, and this halved
the number of results in my database.  We'll see.  The motive here is less
optimization, and more good architecture.


**** Version 6: run table

#!lua
local migration_6 = {}

insert(migrations, migration_6)
#/lua

It has become clear that we need a concept of a 'run', distinct from sessions.

A run is simply everything which happens from starting helm to closing it.

The model for the =input= table is straightforward, although we're doing
deduplication if a line is executed multiple times in a row, we actually
shouldn't: conceptually, =input= is a simple linear collection of lines
executed from the helm.

We /could/ do what we do with reprs, and have a unique =line_text= table for
each unique typed at the repl, but I don't think it's worth the extra level of
indirection.

Runs, at a base level, are a collection of lines, but there is also metadata
we want to preserve.  In particular, the point at which a restart is triggered,
but I would be surprised if that is the full extent of what we want to
preserve.

What is clear enough is that we have two tables, =run= and =run_action=.  We
have one entry in =run= per execution of =helm=, and the contents are stored
in =run_action=.  Since most of these are lines, we want a =input= foreign key,
which can be null, to represent the most common case, and we can probably get
by with one more column (perhaps =run_action.action=?) which is a string which
represents the type of action.  I've been using short, human-readable strings
for this sort of data, but it's probably better to use a single byte of ASCII
and rehydrate the data in-memory.  SQLite doesn't reserve disk space it isn't
using, and there's an appreciable difference between storing ='line'= and
='restart'= versus merely ='l'= and ='r'=.

I'll apply a check constraint here, but limit it to 3 characters instead of
one.  This assures that we put a string in that column, and gives some
headroom for expansion.

Like premises, our best bet is to make the foreign key for =run_action= a
tuple of =(run, ordinal)=, since we'll be recording every meaningful action
in order.

I don't know if there's a way to enforce "every ordinal for a given repr must
be monotonic and increasing, starting with 1" from within SQLite, but it seems
like a common enough pattern and would be nice to have, so I'll look into it.

The run itself is a date and a project, and a foreign key to hang the actions
upon.  We'll also include a third table, =run_attr=, with =(run, key, value)=,
to store metadata in an open-ended fashion.  We can move some of these as
columns onto =run=, if we end up using them consistently, and/or add a JSON
field to =run= itself and fold the map table in, once we have the
prerequisites for working with JSON inside bridge.


***** run tables

  The =run= table itself holds data pertaining to the run, and serves as a
foreign key for =run_action=.

We'll also create a =run_attr= table, which is a classic EAV.

In this case, we're doing it for flexibility in expanding what information we
store about a run, without needing to migrate every time we make changes.

The expectation is that we'll migrate information which we keep for every run
over to the run table itself, and may eventually drop the =run_attr= table
completely.

#!sql @create_run_table #asLua
CREATE TABLE IF NOT EXISTS run (
   run_id INTEGER PRIMARY KEY,
   project INTEGER NOT NULL,
   start_time DATETIME DEFAULT (strftime('%Y-%m-%dT%H:%M:%f', 'now')),
   finish_time DATETIME,
   FOREIGN KEY (project)
      REFERENCES project (project_id)
      ON DELETE CASCADE
);
#/sql

#!sql @create_run_attr_table #asLua
CREATE TABLE IF NOT EXISTS run_attr (
   run_attr_id INTEGER PRIMARY KEY,
   run INTEGER,
   key TEXT,
   value BLOB,
   FOREIGN KEY (run)
      REFERENCES run (run_id)
      ON DELETE CASCADE
);
#/sql


***** run_action tables

  A run action is a single action taken at the repl, such as a line of input,
a restart, entering or exiting session review mode, and so on.

We also create an EAV table here, =action_attr=.  This is a more principled
use of the EAV pattern: run actions are a grab-bag of different things, which
do belong in one table, because actions are a definite thing which happens in
a linear order.

However, each of these actions has some amount of associated data, ranging
from none to several key/value pairs.  Input is the most common type of action,
and the paradigm case, so as an optimization, if the action is an input, we
put the foreign key on the run_action itself.

But joining across a table for each type of action would be fiddly, require
frequent migrations and query maintenance, and probably doesn't do us much of
a favor efficiency wise, although the SQLite query planner is a thing of
beauty and I wouldn't second-guess it.

#!sql @create_run_action_table #asLua
CREATE TABLE IF NOT EXISTS run_action (
   ordinal INTEGER,
   class TEXT CHECK (length(class) <= 3),
   input INTEGER,
   run INTEGER,
   PRIMARY KEY (run, ordinal) -- ON CONFLICT ABORT?
   FOREIGN KEY (run)
      REFERENCES run (run_id)
      ON DELETE CASCADE
   FOREIGN KEY (input)
      REFERENCES input (line_id)
);
#/sql

#!sql @create_action_attr_table #asLua
CREATE TABLE IF NOT EXISTS action_attr (
   action_attr_id PRIMARY KEY,
   run_action INTEGER,
   key TEXT,
   value BLOB,
   FOREIGN KEY (run_action)
      REFERENCES run_action (run_action_id)
      ON DELETE CASCADE
);
#/sql


***** error table

  We're going to start storing everything before "stack traceback:" in an
error, for use in sessions.

That won't live in =results=, but rather in its own error table.

#!sql @create_error_string_table #asLua
CREATE TABLE IF NOT EXISTS error_string (
   error_id INTEGER PRIMARY KEY,
   string TEXT UNIQUE ON CONFLICT IGNORE
);
#/sql

Which we'll want to index:

#!sql @create_error_string_idx #asLua
CREATE INDEX idx_error_string ON error_string (string);
#/sql

This one is all creates, we aren't altering anything we have already.

#!lua
insert(migration_6, create_run_table)
insert(migration_6, create_run_attr_table)
insert(migration_6, create_run_action_table)
insert(migration_6, create_action_attr_table)
insert(migration_6, create_error_string_table)
insert(migration_6, create_error_string_idx)
#/lua


*** Future Migrations

Right now, =helm= is omokase: you get some readline commands, and you get the
colors we give you, and that's that.

In the near future, we intend to add persistent user preferences, starting
with color schemes.

There are a lot of unanswered questions about how to do this in a general way.
The standard approach is config files, and that has a lot to recommend it.

For color schemes, however, we're going to put them in the database, and we
might continue in that vein.  This will take the form of an EAV table called
=preference=, which starts with one value, a foreign key to the
=color_profile= table.  The =color= table is a number of colors with a name
and a foreign key to =color_profile=.

Hmm.  The more I think about this, it seems less than ideal.  The alternative
being a simple orb file with Lua tables in it, which is easier to inspect and
modify.  I want the user to be able to change colors inside =helm= and have
those changes persist, but this isn't impossible or even that difficult to
do with Lua files.

I just wrote a TOML parser for use in the manifest files, so that's a good
candidate for configuration data now.

I dunno. I've been waffling on this for months. Ah well.


** Statement proxy tables

  We retain the conns within the =helm_db= singleton, returning a proxy table
to consumers, which can be indexed to obtain fresh prepared statements.

These tables will also be equipped with functions which close over the conn to
execute operations, particularly transactions, which require the use of
=conn:exec=.

#!lua
local function _prepareStatements(conn, stmts)
   return function(_, key)
      if stmts[key] then
         return conn:prepare(stmts[key])
      else
         error("Don't have a statement " .. key .. " to prepare.")
      end
   end
end

local function _readOnly(_, key, value)
   error ("can't assign to prepared statements table, key: " .. key
          .. " value: " .. value)
end

local lastRowId = assert(sql.lastRowId)
function _makeProxy(conn, stmts)
   return setmetatable({ lastRowId = function() return lastRowId(conn) end },
                       { __index = _prepareStatements(conn, stmts),
                         __newindex = _readOnly })
end
#/lua


*** Historian

  Generates prepared statements and contains closures for the necessary
savepoints to operate [[historian][@:helm/historian]].


**** Historian SQL statements

#!lua
local historian_sql = {}
helm_db.historian_sql = historian_sql
#/lua


***** Insertions

#!sql @historian_sql.insert_line #asLua
INSERT INTO input (project, line) VALUES (:project, :line);
#/sql

#!sql @historian_sql.insert_result_hash #asLua
INSERT INTO result (line_id, hash) VALUES (:line_id, :hash);
#/sql

#!sql @historian_sql.insert_repr #asLua
INSERT INTO repr (hash, repr) VALUES (:hash, :repr);
#/sql

#!sql @historian_sql.insert_project #asLua
INSERT INTO project (directory) VALUES (?);
#/sql

***** Selections

#!sql @historian_sql.get_recent #asLua
SELECT CAST (line_id AS REAL), line FROM input
   WHERE project = :project
   ORDER BY line_id DESC
   LIMIT :num_lines;
#/sql

#!sql @historian_sql.get_number_of_lines #asLua
SELECT CAST (count(line) AS REAL) from input
   WHERE project = ?
;
#/sql

#!sql @historian_sql.get_project #asLua
SELECT project_id FROM project
   WHERE directory = ?;
#/sql

#!sql @historian_sql.get_results #asLua
SELECT repr
FROM result
INNER JOIN repr ON repr.hash == result.hash
WHERE result.line_id = :line_id
ORDER BY result.result_id;
#/sql


**** helm_db.historian(conn?)

  Returns a table of the necessary prepared statements, and closures, for
=historian= to conduct database operations.  =conn= defaults to the system
helm_db.

#!lua
function helm_db.historian(conn_handle)
   local conn = _openConn(conn_handle)
   local hist_proxy = _makeProxy(conn, historian_sql)
   rawset(hist_proxy, "savepoint_persist",
          function()
            conn:exec "SAVEPOINT save_persist"
          end)
   rawset(hist_proxy, "release_persist",
          function()
             conn:exec "RELEASE save_persist"
          end)
   rawset(hist_proxy, "savepoint_restart_session",
          function()
             conn:exec "SAVEPOINT restart_session"
          end)
   rawset(hist_proxy, "release_restart_session",
          function()
             conn:exec "RELEASE restart_session"
          end)
   return hist_proxy
end
#/lua


*** Session

  The =helm_db= singleton is also used by [[valiant][@valiant:session]] to
manage database operations, through the same sort of proxy table as above.

#!lua
local session_sql = {}
#/lua


**** SQL


***** Insertions

#!sql @session_sql.insert_session #asLua
INSERT INTO
   session (title, project, accepted)
VALUES
   (:session_title, :project_id, :accepted)
;
#/sql

#!sql @session_sql.insert_premise #asLua
INSERT INTO
   premise (session, ordinal, line, title, status)
VALUES
   (:session_id, :ordinal, :line_id, :title, :status)
;
#/sql

#!sql @session_sql.truncate_session #asLua
DELETE FROM premise WHERE session = :session_id AND ordinal > :n;
#/sql

#!sql @session_sql.delete_session_by_id #asLua
DELETE FROM session WHERE session_id = :session_id;
#/sql

We need to add lines, including a timestamp, for imports:

#!sql @session_sql.insert_line #asLua
INSERT INTO input (project, line, time) VALUES (:project, :line, :time);
#/sql

We copy over a few statements from the historian proxy table:

#!lua
session_sql.insert_result_hash = historian_sql.insert_result_hash
session_sql.insert_repr        = historian_sql.insert_repr
#/lua


***** Updates

Note that insert_premise can also serve as an update, since the table is
declared with ON CONFLICT REPLACE and we always supply all fields

#!sql @session_sql.update_session #asLua
UPDATE session SET title = :session_title, accepted = :accepted
   WHERE session_id = :session_id;
#/sql


**** Deletions

#!sql @session_sql.delete_session_by_id #asLua
DELETE FROM session WHERE session_id = :session_id;
#/sql

#!sql @session_sql.update_accepted_session #asLua
UPDATE session SET accepted = :accepted WHERE session_id = :session_id;
#/sql

#!sql @session_sql.update_title_session #asLua
UPDATE session SET title = :title WHERE session_id = :session_id;
#/sql

***** Selections

#!sql @session_sql.get_session_by_id #asLua
SELECT
   session.title AS session_title,
   session.accepted AS session_accepted,
   session.session_id,
   session.project,
   premise.status,
   premise.title,
   input.line,
   input.time,
   input.line_id
FROM
   session
LEFT JOIN premise ON premise.session = session.session_id
LEFT JOIN input ON input.line_id = premise.line
WHERE session.session_id = ?
ORDER BY premise.ordinal
;
#/sql

Because the results have a many-to-one relationship with the lines, we're
better off retrieving them separately:

#!sql @session_sql.get_results #asLua
SELECT repr.repr
FROM result
INNER JOIN repr ON result.hash = repr.hash
WHERE result.line_id = ?
ORDER BY result.result_id;
#/sql

Sometimes we just want the session data:

#!sql @session_sql.get_sessions_for_project #asLua
SELECT title as session_title, accepted, project, vc_hash, session_id
FROM session
WHERE session.project = :project_id
ORDER BY session.session_id;
#/sql

#!sql @session_sql.get_project_by_dir #asLua
SELECT project_id FROM project WHERE directory = ?;
#/sql

#!sql @session_sql.get_accepted_by_dir #asLua
SELECT title FROM session
INNER JOIN
   project ON session.project = project.project_id
WHERE
   project.directory = ?
AND
   session.accepted = 1
ORDER BY
   session.session_id
;
#/sql

#!sql @session_sql.get_session_list_by_dir #asLua
SELECT title, accepted, session_id FROM session
INNER JOIN
   project ON session.project = project.project_id
WHERE
   project.directory = ?
ORDER BY
   session.session_id
;
#/sql

#!sql @session_sql.count_premises #asLua
SELECT CAST (count(premise.ordinal) AS REAL)
FROM premise
WHERE session = :session_id
;
#/sql

#!sql @session_sql.get_sessions_from_project #asLua
SELECT
   session_id,
   CAST(accepted AS REAL) As accepted
FROM
   session
WHERE
   project = ?
ORDER BY
   session.session_id
;
#/sql

#!sql @session_sql.get_sessions_by_project #asLua
SELECT session_id FROM session
WHERE project = ?
ORDER BY session_id
;
#/sql

#!sql @session_sql.get_session_by_project_and_title #asLua
SELECT
   CAST (session_id AS REAL) AS session_id,
   CAST (accepted AS REAL) AS accepted
FROM session
WHERE project = ? AND title = ?
ORDER BY session_id
;
#/sql

#!sql @session_sql.get_premises_for_export #asLua
SELECT
   CAST (ordinal AS REAL) AS ordinal,
   premise.title as title,
   premise.status as status,
   input.line as line,
   input.time as time,
   input.line_id as line_id
FROM
   premise
LEFT JOIN
   input
ON
   input.line_id = premise.line
WHERE
   premise.session = :session_id
ORDER BY
   premise.ordinal
;
#/sql

This one is a =conn:exec= so we use a closure, rather than a prepared
statement.

#!sql @session_get_project_info #asLua
SELECT project_id, directory from project;
#/sql

#!sql @session_sql.update_premise_line #asLua
UPDATE premise
SET line = :line
WHERE
   session = :session
AND
   ordinal = :ordinal
;
#/sql

#!lua
function helm_db.session(conn_handle)
   local conn = _openConn(conn_handle)
   local stmts =  _makeProxy(conn, session_sql)
   rawset(stmts, "get_project_info",
          function()
             return conn:exec(session_get_project_info)
          end)
   rawset(stmts, "beginTransaction",
          function()
             return conn:exec "BEGIN TRANSACTION;"
          end)
   rawset(stmts, "commit",
          function()
             return conn:exec "COMMIT;"
          end)
   return stmts
end
#/lua


*** boot(conn_handle?)

=boot= takes an open =historian.conn=, or a file path, and brings it up to
speed.

Returns the conn, or errors and exits the program.

#!lua
local assertfmt = require "core:core/string" . assertfmt
local format = assert(string.format)
local boot = assert(sql.boot)


function helm_db.boot(conn_handle)
   local conn = _resolveConn(conn_handle)
   if not conn then
      conn_handle = helm_db_home
      conn = boot(conn_handle, migrations)
      _conns[conn_handle] = conn
   end

   return conn
end
#/lua


*** close(conn_handle?)

Closes the given conn or conn-string, defaulting to the helm_db_home conn.

#!lua
function helm_db.close(conn_handle)
   local conn = _resolveConn(conn_handle)
   if not conn then
      conn = _conns[helm_db_home]
      conn_handle = helm_db_home
   end
   if not conn then return end
   pcall(conn.pragma.wal_checkpoint, "0") -- 0 == SQLITE_CHECKPOINT_PASSIVE
   -- set up an idler to close the conn, so that e.g. busy
   -- exceptions don't blow up the hook
   local close_idler = uv.new_idle()
   close_idler:start(function()
      local success = pcall(conn.close, conn)
      if not success then
         return nil
      else
         -- we don't want to rely on GC to prevent closing a conn twice
         _conns[conn_handle] = nil
         close_idler:stop()
      end
   end)
end
#/lua


*** conn(conn_handle?)

Retrieve a conn, defaulting as usual to =helm_db_home=, if one already exists.

Primarily for use at the REPL.

#!lua
function helm_db.conn(conn_handle)
   conn_handle = conn_handle or helm_db_home
   return _conns[conn_handle]
end
#/lua


**** protect helm_db

As a singleton, it should be read only.

#!lua
setmetatable(helm_db, { __newindex = function()
                                        error "cannnot assign to helm_db"
                                     end })
#/lua


#!lua
return helm_db
#/lua
